# @package _global_

# 1. Load up the default component configurations
defaults:
  - base_config
  - opt: adamw
  - llm: gpt2.yaml
  - override llm/peft: adalora
  - override llm/quantization: none
  - _self_

# 2. Task-specific configurations:
notes: |-
  Multiple choice symbol binding finetuning.

task_name: mcsb

seed: null

# The number of multiple-choice options in each question
num_labels: 5

# the total number of synthetic examples that will be generated and put to the model
training_examples: 100000

# batch size = micro_batch_size * gradient_accumulation_steps
# note: micro_batch_size must be divisible by number of processes
micro_batch_size: 20
gradient_accumulation_steps: 4
num_warmup_steps: 20

eval:
  freq: 10
  batch_size: 64
  min_labels: 5
  max_labels: 9

checkpoint:
  freq: 100
  keep: 3
# to recover from a given checkpoint, paste the directory path here:
hydra:
  run:
    dir: logs/mcsb/2023-10-27_15-49-09

opt:
  lr: 0.001

# 3. Overload default component configurations for the task at hand
llm:
  use_peft: true

  model_kwargs:
    low_cpu_mem_usage: true
    torch_dtype: bfloat16

  tokenizer_kwargs:
    padding_side: left

  tokenizer_special_tokens:
    pad_token: tokenizer.bos_token

  global_gen_kwargs:
    do_sample: True
