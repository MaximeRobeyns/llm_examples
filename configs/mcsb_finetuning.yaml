# @package _global_

defaults:
  - base_config
  - opt: adamw
  - llm: gpt2.yaml
  - override llm/peft: adalora
  - override llm/quantization: none
  - _self_

notes: |-
  Multiple choice symbol binding finetuning.

# The number of multiple-choice options in each question
num_labels: 5

task_name: mcsb
num_iters: 1000
num_warmup_steps: 10

# batch size = micro_batch_size * gradient_accumulation_steps
# note: micro_batch_size must be divisible by number of processes
micro_batch_size: 20
gradient_accumulation_steps: 4

opt:
  lr: 0.0001

# The following are task-specific modifications / overloads to the llm
# configuration loaded above:
llm:
  use_peft: true

  model_kwargs:
    low_cpu_mem_usage: true
    torch_dtype: bfloat16

  tokenizer_kwargs:
    padding_side: left

  tokenizer_special_tokens:
    pad_token: tokenizer.bos_token

  global_gen_kwargs:
    do_sample: True
